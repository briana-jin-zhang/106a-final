{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f31ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b68ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cada8ae",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a689813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'log'\n",
    "dataset_json = 'dataset.json'\n",
    "data_len = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563e979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce dataset_json\n",
    "n = 0\n",
    "dataset_paths = []\n",
    "for i in range(15000):\n",
    "    if os.path.exists(os.path.join(data_dir, str(i))):\n",
    "        dataset_paths.append(i)\n",
    "        n += 1\n",
    "    if n >= data_len:\n",
    "        break\n",
    "\n",
    "with open(os.path.join(data_dir, dataset_json), 'w') as f:\n",
    "    json.dump({'dataset_paths': dataset_paths}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cf9b1",
   "metadata": {},
   "source": [
    "Note: Training seems to be CPU bound - could try preloading all the data into memory if it's small enough. Alternatively, reduce the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588233d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothSimDataset(Dataset):\n",
    "    \"\"\"Simulated cloth parameters dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, dataset_json, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the data.\n",
    "            data_len (int): Name of the json file with dataset information.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        with open(os.path.join(data_dir, dataset_json)) as f:\n",
    "            dataset_paths = json.load(f)['dataset_paths']\n",
    "        self.samples = [self.load_sample(os.path.join(data_dir, str(path))) for path in dataset_paths]\n",
    "    \n",
    "    def load_sample(self, sample_dir):\n",
    "        # cloth_points = np.load(os.path.join(sample_dir, 'joint_angles.npy'))\n",
    "        cloth_points = np.load(os.path.join(sample_dir, 'cloth_points_transformed.npy'))\n",
    "        parameters = np.load(os.path.join(sample_dir, 'parameters.npy'))\n",
    "        return cloth_points, parameters\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = self.samples[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a768cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample):\n",
    "    cloth_points, params = sample\n",
    "    # Log spring_damper_time_constant\n",
    "    params[0] = np.log(params[0])\n",
    "    # Scale tendon_stiffness\n",
    "    params[3] *= 10\n",
    "    # TODO: Augmentations\n",
    "    # Random crop to 150\n",
    "    # k = int(np.random.random() * 50)\n",
    "    k = 0 # 10\n",
    "    cloth_points = cloth_points[k:k+150]\n",
    "    return cloth_points, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddec4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "train_length = int(train_split * data_len)\n",
    "val_length = data_len - train_length\n",
    "all_dataset = ClothSimDataset(data_dir, dataset_json, transform=transform)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(all_dataset, [train_length, val_length])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121ea135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 150, 5, 2])\n",
      "torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "for pts, params in train_loader:\n",
    "    print(pts.shape)\n",
    "    print(params.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc6f08",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0740b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, (3, 3))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (1, 5))\n",
    "        self.conv4 = nn.Conv2d(128, 128, (1, 5))\n",
    "        self.conv5 = nn.Conv2d(128, 256, (1, 5))\n",
    "        self.conv6 = nn.Conv2d(256, 256, (1, 5))\n",
    "        self.pool = nn.MaxPool2d((1, 2), (1, 2))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(256 * 18, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.permute(x, (0, 3, 2, 1))\n",
    "        # (batch, 2, 5, 200)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # (batch, 64, 1, 98)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        # (batch, 128, 1, 45)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(F.relu(self.conv6(x)))\n",
    "        # (batch, 256, 1, 18)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d63456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 32, 7, stride=3)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 5, stride=1)\n",
    "        self.conv3 = nn.Conv1d(64, 128, 3, stride=1)\n",
    "        self.conv4 = nn.Conv1d(128, 128, 3, stride=1)\n",
    "        self.conv5 = nn.Conv1d(128, 128, 3, stride=1)\n",
    "        self.pool = nn.MaxPool1d(3, stride=2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.fc1 = nn.Linear(128, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 2 * 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = torch.permute(x, (0, 3, 2, 1))\n",
    "        # (batch, 2, 5, 200)\n",
    "#         x = torch.reshape(x, (-1, 10, 150))\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = self.activation(self.conv4(x))\n",
    "        x = self.activation(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a659221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batchNorm = nn.BatchNorm2d(2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(1500, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 2 * 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = torch.permute(x, (0, 3, 2, 1))\n",
    "        # (batch, 2, 5, 200)\n",
    "        # x = self.batchNorm(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9da68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310205ec",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f25d338d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.21679564514756203, Val Loss: 0.1018994805358705\n",
      "Epoch: 1, Train Loss: 0.04795926822721958, Val Loss: 0.0369684174065552\n",
      "Epoch: 2, Train Loss: 0.03602669283747673, Val Loss: 0.03594026821000235\n",
      "Epoch: 3, Train Loss: 0.03554555212706328, Val Loss: 0.035669993402229414\n",
      "Epoch: 4, Train Loss: 0.03552731765061617, Val Loss: 0.03566940371242781\n",
      "Epoch: 5, Train Loss: 0.03547124193608761, Val Loss: 0.03563328536729964\n",
      "Epoch: 6, Train Loss: 0.03548634845763445, Val Loss: 0.03559512065516578\n",
      "Epoch: 7, Train Loss: 0.035487971112132075, Val Loss: 0.03555209828274591\n",
      "Epoch: 8, Train Loss: 0.03542105563730001, Val Loss: 0.03563276595539517\n",
      "Epoch: 9, Train Loss: 0.03544110172241926, Val Loss: 0.035557366878030794\n",
      "Epoch: 10, Train Loss: 0.03544657012820244, Val Loss: 0.035646582288401465\n",
      "Epoch: 11, Train Loss: 0.03539647915959358, Val Loss: 0.0355662585250915\n",
      "Epoch: 12, Train Loss: 0.03533182033151388, Val Loss: 0.03556331797015099\n",
      "Epoch: 13, Train Loss: 0.03530934326350689, Val Loss: 0.03543102844721741\n",
      "Epoch: 14, Train Loss: 0.035289094716310504, Val Loss: 0.035396545001911736\n",
      "Epoch: 15, Train Loss: 0.035230033949017525, Val Loss: 0.035349577458368406\n",
      "Epoch: 16, Train Loss: 0.035207979544997214, Val Loss: 0.03526796784902376\n",
      "Epoch: 17, Train Loss: 0.03503922355175018, Val Loss: 0.03515570391974752\n",
      "Epoch: 18, Train Loss: 0.03489383497089148, Val Loss: 0.03487273888100707\n",
      "Epoch: 19, Train Loss: 0.03467091504484415, Val Loss: 0.0345211670630508\n",
      "Epoch: 20, Train Loss: 0.03421795096248388, Val Loss: 0.033995255709640564\n",
      "Epoch: 21, Train Loss: 0.03373405529558659, Val Loss: 0.033462130656791114\n",
      "Epoch: 22, Train Loss: 0.033109550818800924, Val Loss: 0.0327994300141221\n",
      "Epoch: 23, Train Loss: 0.03237763952463865, Val Loss: 0.032128223824122594\n",
      "Epoch: 24, Train Loss: 0.03180423350632191, Val Loss: 0.03165745681950024\n",
      "Epoch: 25, Train Loss: 0.03135764141380787, Val Loss: 0.03125499679692208\n",
      "Epoch: 26, Train Loss: 0.031040677316486836, Val Loss: 0.030908215465763257\n",
      "Epoch: 27, Train Loss: 0.030771418184041978, Val Loss: 0.030690838834123005\n",
      "Epoch: 28, Train Loss: 0.030508324146270752, Val Loss: 0.030469952299008295\n",
      "Epoch: 29, Train Loss: 0.03030103651434183, Val Loss: 0.03036149461118002\n",
      "Epoch: 30, Train Loss: 0.03017053596675396, Val Loss: 0.03022279492801144\n",
      "Epoch: 31, Train Loss: 0.029927846930921077, Val Loss: 0.030065176358062124\n",
      "Epoch: 32, Train Loss: 0.029853857025504114, Val Loss: 0.029969515722422374\n",
      "Epoch: 33, Train Loss: 0.029712922915816307, Val Loss: 0.029799605793659648\n",
      "Epoch: 34, Train Loss: 0.02960712465643883, Val Loss: 0.029789992682044467\n",
      "Epoch: 35, Train Loss: 0.02942219065874815, Val Loss: 0.02955301239022187\n",
      "Epoch: 36, Train Loss: 0.029318332679569722, Val Loss: 0.029479911343918905\n",
      "Epoch: 37, Train Loss: 0.029214294590055943, Val Loss: 0.029329088797408438\n",
      "Epoch: 38, Train Loss: 0.029106763824820518, Val Loss: 0.029302593645831897\n",
      "Epoch: 39, Train Loss: 0.028966221623122693, Val Loss: 0.029199303231305547\n",
      "Epoch: 40, Train Loss: 0.0288330008238554, Val Loss: 0.028994785917420236\n",
      "Epoch: 41, Train Loss: 0.028657440945506096, Val Loss: 0.028913904838855305\n",
      "Epoch: 42, Train Loss: 0.028474154740571977, Val Loss: 0.028738935906735676\n",
      "Epoch: 43, Train Loss: 0.028298705734312535, Val Loss: 0.02855803430198677\n",
      "Epoch: 44, Train Loss: 0.02809462694078684, Val Loss: 0.02839765866242704\n",
      "Epoch: 45, Train Loss: 0.027851089775562287, Val Loss: 0.028100747910756913\n",
      "Epoch: 46, Train Loss: 0.027468620039522648, Val Loss: 0.027661267993232562\n",
      "Epoch: 47, Train Loss: 0.027079992473125456, Val Loss: 0.027277980147609637\n",
      "Epoch: 48, Train Loss: 0.026573031298816205, Val Loss: 0.026725741576344247\n",
      "Epoch: 49, Train Loss: 0.02596374338120222, Val Loss: 0.026132346530045782\n",
      "Epoch: 50, Train Loss: 0.02537735888361931, Val Loss: 0.02573113731803402\n",
      "Epoch: 51, Train Loss: 0.024991349183022975, Val Loss: 0.025300585472631077\n",
      "Epoch: 52, Train Loss: 0.024669053040444852, Val Loss: 0.025095556108724503\n",
      "Epoch: 53, Train Loss: 0.024499073684215545, Val Loss: 0.02503007614896411\n",
      "Epoch: 54, Train Loss: 0.024499077431857585, Val Loss: 0.0248899000682055\n",
      "Epoch: 55, Train Loss: 0.02436752226948738, Val Loss: 0.02483392876410295\n",
      "Epoch: 56, Train Loss: 0.02431933433562517, Val Loss: 0.024806701415587985\n",
      "Epoch: 57, Train Loss: 0.02428275363892317, Val Loss: 0.02477618572967393\n",
      "Epoch: 58, Train Loss: 0.024225770093500614, Val Loss: 0.024708442626491425\n",
      "Epoch: 59, Train Loss: 0.02421393869817257, Val Loss: 0.02469098594571863\n",
      "Epoch: 60, Train Loss: 0.02416226167976856, Val Loss: 0.02462258580185118\n",
      "Epoch: 61, Train Loss: 0.024135314680635928, Val Loss: 0.024649148629534812\n",
      "Epoch: 62, Train Loss: 0.024130537644028664, Val Loss: 0.024588532893667147\n",
      "Epoch: 63, Train Loss: 0.024103557236492632, Val Loss: 0.024610998670733165\n",
      "Epoch: 64, Train Loss: 0.024106359221041204, Val Loss: 0.024573298230294197\n",
      "Epoch: 65, Train Loss: 0.02410309650748968, Val Loss: 0.024592508369731526\n",
      "Epoch: 66, Train Loss: 0.02407975070178509, Val Loss: 0.024546367397147512\n",
      "Epoch: 67, Train Loss: 0.0240594180226326, Val Loss: 0.024508212027805194\n",
      "Epoch: 68, Train Loss: 0.024015020161867143, Val Loss: 0.024483056265919928\n",
      "Epoch: 69, Train Loss: 0.02398929550498724, Val Loss: 0.024514811468266305\n",
      "Epoch: 70, Train Loss: 0.02399071303755045, Val Loss: 0.02446967644232606\n",
      "Epoch: 71, Train Loss: 0.023994427151978016, Val Loss: 0.024442068729845303\n",
      "Epoch: 72, Train Loss: 0.023955231867730617, Val Loss: 0.024440310304127043\n",
      "Epoch: 73, Train Loss: 0.0239097683429718, Val Loss: 0.02439661862121688\n",
      "Epoch: 74, Train Loss: 0.023893693543970584, Val Loss: 0.024437537712473718\n",
      "Epoch: 75, Train Loss: 0.023897304765880107, Val Loss: 0.024399678385446943\n",
      "Epoch: 76, Train Loss: 0.023872466184198856, Val Loss: 0.024389905618533256\n",
      "Epoch: 77, Train Loss: 0.023867626696825028, Val Loss: 0.024481143358917462\n",
      "Epoch: 78, Train Loss: 0.023856039464473726, Val Loss: 0.024333644776590287\n",
      "Epoch: 79, Train Loss: 0.023845032900571824, Val Loss: 0.024337913308824812\n",
      "Epoch: 80, Train Loss: 0.02380919924378395, Val Loss: 0.024335514281004195\n",
      "Epoch: 81, Train Loss: 0.023810126706957816, Val Loss: 0.02427035091178758\n",
      "Epoch: 82, Train Loss: 0.02379585259407759, Val Loss: 0.02436144327715276\n",
      "Epoch: 83, Train Loss: 0.023758316777646542, Val Loss: 0.024269077128597667\n",
      "Epoch: 84, Train Loss: 0.02376343135535717, Val Loss: 0.024248321644133992\n",
      "Epoch: 85, Train Loss: 0.023729756973683833, Val Loss: 0.02427518004108043\n",
      "Epoch: 86, Train Loss: 0.023732058107852935, Val Loss: 0.024226268782975183\n",
      "Epoch: 87, Train Loss: 0.023713357962667943, Val Loss: 0.0242160651832819\n",
      "Epoch: 88, Train Loss: 0.023692386262118816, Val Loss: 0.02428010901406644\n",
      "Epoch: 89, Train Loss: 0.02370173119753599, Val Loss: 0.024299040850665834\n",
      "Epoch: 90, Train Loss: 0.02369087141007185, Val Loss: 0.02417514215977419\n",
      "Epoch: 91, Train Loss: 0.023673130646348, Val Loss: 0.024200009063832344\n",
      "Epoch: 92, Train Loss: 0.02363865848630667, Val Loss: 0.02414797550983845\n",
      "Epoch: 93, Train Loss: 0.02364076042920351, Val Loss: 0.02419676338987691\n",
      "Epoch: 94, Train Loss: 0.023606825776398183, Val Loss: 0.02412801135390524\n",
      "Epoch: 95, Train Loss: 0.023608019582927228, Val Loss: 0.0241149091235702\n",
      "Epoch: 96, Train Loss: 0.023577390901744367, Val Loss: 0.024099572753858944\n",
      "Epoch: 97, Train Loss: 0.023563835702836513, Val Loss: 0.024110736621041146\n",
      "Epoch: 98, Train Loss: 0.023576374724507333, Val Loss: 0.024056160112931615\n",
      "Epoch: 99, Train Loss: 0.023551740899682046, Val Loss: 0.02420508666407494\n",
      "Epoch: 100, Train Loss: 0.023542813159525396, Val Loss: 0.024074564111374673\n",
      "Epoch: 101, Train Loss: 0.02350851510465145, Val Loss: 0.024033441430046446\n",
      "Epoch: 102, Train Loss: 0.023495223455131053, Val Loss: 0.024031777999230793\n",
      "Epoch: 103, Train Loss: 0.023490933001041412, Val Loss: 0.02406927523395372\n",
      "Epoch: 104, Train Loss: 0.0234519075229764, Val Loss: 0.02400417241548735\n",
      "Epoch: 105, Train Loss: 0.023444371931254863, Val Loss: 0.02406972495927697\n",
      "Epoch: 106, Train Loss: 0.023459039628505707, Val Loss: 0.024016735365702993\n",
      "Epoch: 107, Train Loss: 0.023428995832800864, Val Loss: 0.023961370396945212\n",
      "Epoch: 108, Train Loss: 0.02340311434864998, Val Loss: 0.023925611778857218\n",
      "Epoch: 109, Train Loss: 0.023393887475132942, Val Loss: 0.023928391466301584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Train Loss: 0.023382183134555815, Val Loss: 0.023928498701443746\n",
      "Epoch: 111, Train Loss: 0.0233770055398345, Val Loss: 0.023906979857692644\n",
      "Epoch: 112, Train Loss: 0.023354918479919433, Val Loss: 0.02390458236729342\n",
      "Epoch: 113, Train Loss: 0.023330030985176563, Val Loss: 0.023907115594262167\n",
      "Epoch: 114, Train Loss: 0.02334356663376093, Val Loss: 0.023875781793206458\n",
      "Epoch: 115, Train Loss: 0.023323530964553356, Val Loss: 0.023880185470694586\n",
      "Epoch: 116, Train Loss: 0.023294346913695335, Val Loss: 0.023859716301399565\n",
      "Epoch: 117, Train Loss: 0.02328594943881035, Val Loss: 0.02382549430642809\n",
      "Epoch: 118, Train Loss: 0.023273233704268932, Val Loss: 0.0238175562154206\n",
      "Epoch: 119, Train Loss: 0.023266232021152973, Val Loss: 0.02384394265356518\n",
      "Epoch: 120, Train Loss: 0.023242437362670898, Val Loss: 0.023807938905462386\n",
      "Epoch: 121, Train Loss: 0.023244905941188335, Val Loss: 0.023847103444120242\n",
      "Epoch: 122, Train Loss: 0.02320045643299818, Val Loss: 0.02381970023825055\n",
      "Epoch: 123, Train Loss: 0.023190077118575573, Val Loss: 0.02376845869280043\n",
      "Epoch: 124, Train Loss: 0.023161776088178158, Val Loss: 0.023731601025377\n",
      "Epoch: 125, Train Loss: 0.023166545212268828, Val Loss: 0.023724034694688662\n",
      "Epoch: 126, Train Loss: 0.0231596063002944, Val Loss: 0.023695320925778814\n",
      "Epoch: 127, Train Loss: 0.02313316658884287, Val Loss: 0.02372996707165998\n",
      "Epoch: 128, Train Loss: 0.023136278934776783, Val Loss: 0.023693513361707566\n",
      "Epoch: 129, Train Loss: 0.023082031190395355, Val Loss: 0.023676940964327917\n",
      "Epoch: 130, Train Loss: 0.023088147766888143, Val Loss: 0.023697982409170697\n",
      "Epoch: 131, Train Loss: 0.023073237963020802, Val Loss: 0.023651053951609703\n",
      "Epoch: 132, Train Loss: 0.02304322998225689, Val Loss: 0.023654180771065136\n",
      "Epoch: 133, Train Loss: 0.023044724240899084, Val Loss: 0.023684526926704814\n",
      "Epoch: 134, Train Loss: 0.02300821965932846, Val Loss: 0.023602097664797116\n",
      "Epoch: 135, Train Loss: 0.02301448528468609, Val Loss: 0.02362349419485009\n",
      "Epoch: 136, Train Loss: 0.02298575483262539, Val Loss: 0.023585896968605028\n",
      "Epoch: 137, Train Loss: 0.022989631935954095, Val Loss: 0.02356141403553978\n",
      "Epoch: 138, Train Loss: 0.02294614962488413, Val Loss: 0.02355005784285447\n",
      "Epoch: 139, Train Loss: 0.02294035392254591, Val Loss: 0.023522363709551946\n",
      "Epoch: 140, Train Loss: 0.022911127507686615, Val Loss: 0.023518065522824014\n",
      "Epoch: 141, Train Loss: 0.022910532400012017, Val Loss: 0.023537443595982734\n",
      "Epoch: 142, Train Loss: 0.02288729929924011, Val Loss: 0.023508413177397516\n",
      "Epoch: 143, Train Loss: 0.02287739310413599, Val Loss: 0.023467542485348762\n",
      "Epoch: 144, Train Loss: 0.02285544813424349, Val Loss: 0.02347535477389419\n",
      "Epoch: 145, Train Loss: 0.022820054322481156, Val Loss: 0.023441754636310395\n",
      "Epoch: 146, Train Loss: 0.022802275091409682, Val Loss: 0.02345783476318632\n",
      "Epoch: 147, Train Loss: 0.022806647159159185, Val Loss: 0.023419325550397236\n",
      "Epoch: 148, Train Loss: 0.022759202383458615, Val Loss: 0.023413287091349797\n",
      "Epoch: 149, Train Loss: 0.02276274697482586, Val Loss: 0.023417550360872633\n",
      "Epoch: 150, Train Loss: 0.022739957585930823, Val Loss: 0.023386771598505596\n",
      "Epoch: 151, Train Loss: 0.02270011331140995, Val Loss: 0.023401060126840123\n",
      "Epoch: 152, Train Loss: 0.022698089517652988, Val Loss: 0.023290704965354906\n",
      "Epoch: 153, Train Loss: 0.02266476546227932, Val Loss: 0.023322224380478027\n",
      "Epoch: 154, Train Loss: 0.022646705761551856, Val Loss: 0.02328930620754522\n",
      "Epoch: 155, Train Loss: 0.022596755035221575, Val Loss: 0.023353596676200156\n",
      "Epoch: 156, Train Loss: 0.022578931637108324, Val Loss: 0.02324372095366319\n",
      "Epoch: 157, Train Loss: 0.022554701067507267, Val Loss: 0.02320600076327248\n",
      "Epoch: 158, Train Loss: 0.022530648827552794, Val Loss: 0.023186529705685282\n",
      "Epoch: 159, Train Loss: 0.022518557511270045, Val Loss: 0.02314921072314656\n",
      "Epoch: 160, Train Loss: 0.02245961371064186, Val Loss: 0.023105066713123096\n",
      "Epoch: 161, Train Loss: 0.022442668572068215, Val Loss: 0.023089381141795054\n",
      "Epoch: 162, Train Loss: 0.022421349115669727, Val Loss: 0.023094440677336285\n",
      "Epoch: 163, Train Loss: 0.02241409754753113, Val Loss: 0.023054585333854433\n",
      "Epoch: 164, Train Loss: 0.02237989705055952, Val Loss: 0.023004382671344848\n",
      "Epoch: 165, Train Loss: 0.022353932343423366, Val Loss: 0.022976860581409363\n",
      "Epoch: 166, Train Loss: 0.022338801179081203, Val Loss: 0.022981765162613656\n",
      "Epoch: 167, Train Loss: 0.02228543648868799, Val Loss: 0.022933240891212507\n",
      "Epoch: 168, Train Loss: 0.022262629590928555, Val Loss: 0.022923044150783903\n",
      "Epoch: 169, Train Loss: 0.022241460278630255, Val Loss: 0.022873471446690104\n",
      "Epoch: 170, Train Loss: 0.022196711979806424, Val Loss: 0.022856443588222777\n",
      "Epoch: 171, Train Loss: 0.022181901067495347, Val Loss: 0.02282682005020361\n",
      "Epoch: 172, Train Loss: 0.022158830426633358, Val Loss: 0.022804770352584974\n",
      "Epoch: 173, Train Loss: 0.02210448081791401, Val Loss: 0.022852768885001304\n",
      "Epoch: 174, Train Loss: 0.022087759353220462, Val Loss: 0.0227477021574501\n",
      "Epoch: 175, Train Loss: 0.02208022826910019, Val Loss: 0.022714939944091298\n",
      "Epoch: 176, Train Loss: 0.02205435099452734, Val Loss: 0.022718817261712893\n",
      "Epoch: 177, Train Loss: 0.022016240499913693, Val Loss: 0.022646092381032688\n",
      "Epoch: 178, Train Loss: 0.021966342486441134, Val Loss: 0.022646082299096242\n",
      "Epoch: 179, Train Loss: 0.021962649427354337, Val Loss: 0.022603685834578106\n",
      "Epoch: 180, Train Loss: 0.021930279046297074, Val Loss: 0.022643991317304355\n",
      "Epoch: 181, Train Loss: 0.021903353355824946, Val Loss: 0.022547088001692105\n",
      "Epoch: 182, Train Loss: 0.02184588932991028, Val Loss: 0.02251612471919211\n",
      "Epoch: 183, Train Loss: 0.021843655847012998, Val Loss: 0.022536816045878424\n",
      "Epoch: 184, Train Loss: 0.021835330456495286, Val Loss: 0.022461737077387554\n",
      "Epoch: 185, Train Loss: 0.021787272315472363, Val Loss: 0.02245823373751981\n",
      "Epoch: 186, Train Loss: 0.021770786844193934, Val Loss: 0.02239843484546457\n",
      "Epoch: 187, Train Loss: 0.021750451900064947, Val Loss: 0.02238677211460613\n",
      "Epoch: 188, Train Loss: 0.021731529749929906, Val Loss: 0.022412301174231937\n",
      "Epoch: 189, Train Loss: 0.021700268127024174, Val Loss: 0.022324407059285376\n",
      "Epoch: 190, Train Loss: 0.02167668740451336, Val Loss: 0.022311098045772977\n",
      "Epoch: 191, Train Loss: 0.021653424136340617, Val Loss: 0.02231290220977768\n",
      "Epoch: 192, Train Loss: 0.021629404820501803, Val Loss: 0.02229053926255022\n",
      "Epoch: 193, Train Loss: 0.02162034022808075, Val Loss: 0.02224309742450714\n",
      "Epoch: 194, Train Loss: 0.021603480596095324, Val Loss: 0.02223252664719309\n",
      "Epoch: 195, Train Loss: 0.02159033803641796, Val Loss: 0.022235568464984968\n",
      "Epoch: 196, Train Loss: 0.021568710118532182, Val Loss: 0.022218485761966025\n",
      "Epoch: 197, Train Loss: 0.021550915092229842, Val Loss: 0.02220509582686992\n",
      "Epoch: 198, Train Loss: 0.02153020728379488, Val Loss: 0.022236323841507474\n",
      "Epoch: 199, Train Loss: 0.021510611824691297, Val Loss: 0.02216753295607983\n",
      "Epoch: 200, Train Loss: 0.02150177252665162, Val Loss: 0.022166520475395142\n",
      "Epoch: 201, Train Loss: 0.021495710365474225, Val Loss: 0.022137555190258555\n",
      "Epoch: 202, Train Loss: 0.021484853334724902, Val Loss: 0.022098441061282916\n",
      "Epoch: 203, Train Loss: 0.021461551919579505, Val Loss: 0.022120201871508642\n",
      "Epoch: 204, Train Loss: 0.021450835287570955, Val Loss: 0.02212362547242452\n",
      "Epoch: 205, Train Loss: 0.02143246454000473, Val Loss: 0.022049642064505152\n",
      "Epoch: 206, Train Loss: 0.021406720913946627, Val Loss: 0.022054966893934068\n",
      "Epoch: 207, Train Loss: 0.021407174721360205, Val Loss: 0.022029723321634626\n",
      "Epoch: 208, Train Loss: 0.021377850420773028, Val Loss: 0.022023006179739558\n",
      "Epoch: 209, Train Loss: 0.02138792261481285, Val Loss: 0.021995232662274725\n",
      "Epoch: 210, Train Loss: 0.021364816643297674, Val Loss: 0.02199257132671182\n",
      "Epoch: 211, Train Loss: 0.021342632375657557, Val Loss: 0.02198298727827413\n",
      "Epoch: 212, Train Loss: 0.021330640204250812, Val Loss: 0.021986948592322215\n",
      "Epoch: 213, Train Loss: 0.021307143032550813, Val Loss: 0.02194927946206123\n",
      "Epoch: 214, Train Loss: 0.02130546221882105, Val Loss: 0.021998591898452668\n",
      "Epoch: 215, Train Loss: 0.0212929121106863, Val Loss: 0.021961726248264313\n",
      "Epoch: 216, Train Loss: 0.02126070938631892, Val Loss: 0.02197027877564468\n",
      "Epoch: 217, Train Loss: 0.02126757351309061, Val Loss: 0.02188292533040993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 218, Train Loss: 0.02125199121236801, Val Loss: 0.02186653467397841\n",
      "Epoch: 219, Train Loss: 0.021229694098234178, Val Loss: 0.021843570801946852\n",
      "Epoch: 220, Train Loss: 0.021215384796261786, Val Loss: 0.02187722581364806\n",
      "Epoch: 221, Train Loss: 0.02122132404893637, Val Loss: 0.021869508342610464\n",
      "Epoch: 222, Train Loss: 0.02121226565539837, Val Loss: 0.021854043746041874\n",
      "Epoch: 223, Train Loss: 0.02118059601634741, Val Loss: 0.021829208388688074\n",
      "Epoch: 224, Train Loss: 0.021180543959140777, Val Loss: 0.021802061577401464\n",
      "Epoch: 225, Train Loss: 0.021173080436885358, Val Loss: 0.021864317239276947\n",
      "Epoch: 226, Train Loss: 0.02112855713814497, Val Loss: 0.021781360789660423\n",
      "Epoch: 227, Train Loss: 0.021140465334057806, Val Loss: 0.021827033321772302\n",
      "Epoch: 228, Train Loss: 0.021141342356801034, Val Loss: 0.021771100778428334\n",
      "Epoch: 229, Train Loss: 0.02110091270506382, Val Loss: 0.02175767385652141\n",
      "Epoch: 230, Train Loss: 0.021098192252218723, Val Loss: 0.02172716325592427\n",
      "Epoch: 231, Train Loss: 0.021089457876980303, Val Loss: 0.021763049332158908\n",
      "Epoch: 232, Train Loss: 0.02109229825437069, Val Loss: 0.021772377399934664\n",
      "Epoch: 233, Train Loss: 0.021072980284690856, Val Loss: 0.02174762110151942\n",
      "Epoch: 234, Train Loss: 0.021074739806354045, Val Loss: 0.02169716100962389\n",
      "Epoch: 235, Train Loss: 0.021026816617697477, Val Loss: 0.02169663142708559\n",
      "Epoch: 236, Train Loss: 0.021037391126155852, Val Loss: 0.021716161113646295\n",
      "Epoch: 237, Train Loss: 0.0210254363194108, Val Loss: 0.021675274544765078\n",
      "Epoch: 238, Train Loss: 0.02101806054264307, Val Loss: 0.02172014064022473\n",
      "Epoch: 239, Train Loss: 0.0210098724886775, Val Loss: 0.021676155428091686\n",
      "Epoch: 240, Train Loss: 0.020985379949212075, Val Loss: 0.02163902906671403\n",
      "Epoch: 241, Train Loss: 0.020961579978466035, Val Loss: 0.021619659804162524\n",
      "Epoch: 242, Train Loss: 0.020973511815071105, Val Loss: 0.021612764203122685\n",
      "Epoch: 243, Train Loss: 0.02095155057311058, Val Loss: 0.02161115759776698\n",
      "Epoch: 244, Train Loss: 0.020946627542376518, Val Loss: 0.021600891289020343\n",
      "Epoch: 245, Train Loss: 0.020921857334673406, Val Loss: 0.021623967008458242\n",
      "Epoch: 246, Train Loss: 0.020926175724714995, Val Loss: 0.021568689436193497\n",
      "Epoch: 247, Train Loss: 0.02091265507042408, Val Loss: 0.02159703579095621\n",
      "Epoch: 248, Train Loss: 0.020889668054878713, Val Loss: 0.021531522924464846\n",
      "Epoch: 249, Train Loss: 0.020900337755680086, Val Loss: 0.02157451892419467\n",
      "Epoch: 250, Train Loss: 0.020868250228464603, Val Loss: 0.021536631125306325\n",
      "Epoch: 251, Train Loss: 0.02085821131616831, Val Loss: 0.02156518180928533\n",
      "Epoch: 252, Train Loss: 0.02084251379966736, Val Loss: 0.021498238904372094\n",
      "Epoch: 253, Train Loss: 0.02085394314676523, Val Loss: 0.021494792951714425\n",
      "Epoch: 254, Train Loss: 0.020827278189361094, Val Loss: 0.0214760158329256\n",
      "Epoch: 255, Train Loss: 0.020818183802068232, Val Loss: 0.021455378877738164\n",
      "Epoch: 256, Train Loss: 0.020814758971333502, Val Loss: 0.02149527549507126\n",
      "Epoch: 257, Train Loss: 0.02080213425308466, Val Loss: 0.02149430943268632\n",
      "Epoch: 258, Train Loss: 0.0207961281016469, Val Loss: 0.021496147331264284\n",
      "Epoch: 259, Train Loss: 0.020779526852071285, Val Loss: 0.021472275286676393\n",
      "Epoch: 260, Train Loss: 0.020756019085645675, Val Loss: 0.021502514473266073\n",
      "Epoch: 261, Train Loss: 0.02075785378366709, Val Loss: 0.021416332247474835\n",
      "Epoch: 262, Train Loss: 0.02074366194009781, Val Loss: 0.021476386233218132\n",
      "Epoch: 263, Train Loss: 0.020727673217654227, Val Loss: 0.02136550225790531\n",
      "Epoch: 264, Train Loss: 0.020720817752182482, Val Loss: 0.021335885253927066\n",
      "Epoch: 265, Train Loss: 0.020709189102053643, Val Loss: 0.02140580392664387\n",
      "Epoch: 266, Train Loss: 0.02066702385991812, Val Loss: 0.021393772983361804\n",
      "Epoch: 267, Train Loss: 0.020684714861214162, Val Loss: 0.02136564441025257\n",
      "Epoch: 268, Train Loss: 0.020672763507813215, Val Loss: 0.02138857315811846\n",
      "Epoch: 269, Train Loss: 0.02063607506453991, Val Loss: 0.021377146391878054\n",
      "Epoch: 270, Train Loss: 0.020636682085692883, Val Loss: 0.021309237306316692\n",
      "Epoch: 271, Train Loss: 0.020620018228888513, Val Loss: 0.021303543968806193\n",
      "Epoch: 272, Train Loss: 0.020612247608602047, Val Loss: 0.021277372769656636\n",
      "Epoch: 273, Train Loss: 0.02058617268502712, Val Loss: 0.02139304545781915\n",
      "Epoch: 274, Train Loss: 0.020589117761701345, Val Loss: 0.021223292641696475\n",
      "Epoch: 275, Train Loss: 0.020557392962276936, Val Loss: 0.021281072012488803\n",
      "Epoch: 276, Train Loss: 0.020557994835078718, Val Loss: 0.021254117822363264\n",
      "Epoch: 277, Train Loss: 0.020557459443807602, Val Loss: 0.02121848991465947\n",
      "Epoch: 278, Train Loss: 0.020582942701876165, Val Loss: 0.021205517773826916\n",
      "Epoch: 279, Train Loss: 0.020548011787235736, Val Loss: 0.02120045744000919\n",
      "Epoch: 280, Train Loss: 0.02053331235796213, Val Loss: 0.021172597265196224\n",
      "Epoch: 281, Train Loss: 0.02053661857172847, Val Loss: 0.021168530608216923\n",
      "Epoch: 282, Train Loss: 0.02050398751720786, Val Loss: 0.02115777607948061\n",
      "Epoch: 283, Train Loss: 0.02050378243625164, Val Loss: 0.021152539622216\n",
      "Epoch: 284, Train Loss: 0.020498461328446866, Val Loss: 0.02121163827795831\n",
      "Epoch: 285, Train Loss: 0.02051179687678814, Val Loss: 0.021230056024496517\n",
      "Epoch: 286, Train Loss: 0.02048360400646925, Val Loss: 0.021131064949764147\n",
      "Epoch: 287, Train Loss: 0.020469335038214923, Val Loss: 0.02118522280620204\n",
      "Epoch: 288, Train Loss: 0.020474375545978547, Val Loss: 0.021167713439180738\n",
      "Epoch: 289, Train Loss: 0.020442523293197155, Val Loss: 0.021197155798001896\n",
      "Epoch: 290, Train Loss: 0.020460810847580432, Val Loss: 0.02114830186797513\n",
      "Epoch: 291, Train Loss: 0.020444590710103513, Val Loss: 0.021100544681151707\n",
      "Epoch: 292, Train Loss: 0.02042299161851406, Val Loss: 0.02107188794466238\n",
      "Epoch: 293, Train Loss: 0.020422357227653266, Val Loss: 0.021074507060268567\n",
      "Epoch: 294, Train Loss: 0.02042016876488924, Val Loss: 0.02107396124610825\n",
      "Epoch: 295, Train Loss: 0.020395198974758385, Val Loss: 0.021035845731458967\n",
      "Epoch: 296, Train Loss: 0.020395049888640642, Val Loss: 0.02106815798296815\n",
      "Epoch: 297, Train Loss: 0.02038862607255578, Val Loss: 0.02110007396411328\n",
      "Epoch: 298, Train Loss: 0.020398459777235985, Val Loss: 0.021017348985113794\n",
      "Epoch: 299, Train Loss: 0.02038902909308672, Val Loss: 0.021016813903337435\n",
      "Epoch: 300, Train Loss: 0.02038090246170759, Val Loss: 0.021106965958126007\n",
      "Epoch: 301, Train Loss: 0.020361366987228395, Val Loss: 0.02102120528145442\n",
      "Epoch: 302, Train Loss: 0.02033693664893508, Val Loss: 0.021015301465042054\n",
      "Epoch: 303, Train Loss: 0.02037441248446703, Val Loss: 0.02100370620333013\n",
      "Epoch: 304, Train Loss: 0.020365831296890975, Val Loss: 0.020968869387630432\n",
      "Epoch: 305, Train Loss: 0.020353913456201553, Val Loss: 0.020988391061860418\n",
      "Epoch: 306, Train Loss: 0.020333348426967858, Val Loss: 0.02097676321864128\n",
      "Epoch: 307, Train Loss: 0.020333441730588674, Val Loss: 0.021135392347498547\n",
      "Epoch: 308, Train Loss: 0.020323007587343456, Val Loss: 0.02103999579354884\n",
      "Epoch: 309, Train Loss: 0.020317023191601037, Val Loss: 0.0210565250248663\n",
      "Epoch: 310, Train Loss: 0.02030619315057993, Val Loss: 0.020965592107839055\n",
      "Epoch: 311, Train Loss: 0.020292553402483465, Val Loss: 0.02100787897195135\n",
      "Epoch: 312, Train Loss: 0.02029091838002205, Val Loss: 0.020906978922467383\n",
      "Epoch: 313, Train Loss: 0.020284749541431665, Val Loss: 0.020920014274971827\n",
      "Epoch: 314, Train Loss: 0.020284713722765444, Val Loss: 0.020939494024903055\n",
      "Epoch: 315, Train Loss: 0.020272327478975057, Val Loss: 0.020999759317390503\n",
      "Epoch: 316, Train Loss: 0.020244686357676983, Val Loss: 0.02092176838408387\n",
      "Epoch: 317, Train Loss: 0.020258980453014375, Val Loss: 0.020957822719263653\n",
      "Epoch: 318, Train Loss: 0.020242146205157043, Val Loss: 0.02091852120227284\n",
      "Epoch: 319, Train Loss: 0.02025647897273302, Val Loss: 0.020924095833112322\n",
      "Epoch: 320, Train Loss: 0.020239058934152128, Val Loss: 0.020851159734385356\n",
      "Epoch: 321, Train Loss: 0.020211716026067732, Val Loss: 0.020963112217566325\n",
      "Epoch: 322, Train Loss: 0.02020930928736925, Val Loss: 0.020868329065186635\n",
      "Epoch: 323, Train Loss: 0.020225522734224795, Val Loss: 0.020958777901435657\n",
      "Epoch: 324, Train Loss: 0.02022675462067127, Val Loss: 0.020949973798696956\n",
      "Epoch: 325, Train Loss: 0.020231404311954974, Val Loss: 0.020865091787917272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 326, Train Loss: 0.020191628336906433, Val Loss: 0.020820195505779886\n",
      "Epoch: 327, Train Loss: 0.02020620412006974, Val Loss: 0.020827523950073455\n",
      "Epoch: 328, Train Loss: 0.0202105422988534, Val Loss: 0.020800134699259485\n",
      "Epoch: 329, Train Loss: 0.02017990366369486, Val Loss: 0.02080985770693847\n",
      "Epoch: 330, Train Loss: 0.020175499331206083, Val Loss: 0.020850573503781878\n",
      "Epoch: 331, Train Loss: 0.020163309879601, Val Loss: 0.020920377135986372\n",
      "Epoch: 332, Train Loss: 0.020137317448854445, Val Loss: 0.02079941604345564\n",
      "Epoch: 333, Train Loss: 0.02015881073474884, Val Loss: 0.02081688106178291\n",
      "Epoch: 334, Train Loss: 0.02013536899536848, Val Loss: 0.02082688816719585\n",
      "Epoch: 335, Train Loss: 0.02015893155708909, Val Loss: 0.020761107818947896\n",
      "Epoch: 336, Train Loss: 0.02012306210398674, Val Loss: 0.020883754782733462\n",
      "Epoch: 337, Train Loss: 0.020154932945966722, Val Loss: 0.020757762212601918\n",
      "Epoch: 338, Train Loss: 0.02012091786786914, Val Loss: 0.020752943579166655\n",
      "Epoch: 339, Train Loss: 0.02013811018317938, Val Loss: 0.020809488311883003\n",
      "Epoch: 340, Train Loss: 0.020124583300203086, Val Loss: 0.020728160050653276\n",
      "Epoch: 341, Train Loss: 0.020108852617442607, Val Loss: 0.0207169974843661\n",
      "Epoch: 342, Train Loss: 0.020115717373788356, Val Loss: 0.020729985915952258\n",
      "Epoch: 343, Train Loss: 0.020121112875640394, Val Loss: 0.02070963746380238\n",
      "Epoch: 344, Train Loss: 0.020102936681360005, Val Loss: 0.02075140175247003\n",
      "Epoch: 345, Train Loss: 0.020095511067658663, Val Loss: 0.020773824156513288\n",
      "Epoch: 346, Train Loss: 0.020066469579935072, Val Loss: 0.020672286431940776\n",
      "Epoch: 347, Train Loss: 0.020077522251755, Val Loss: 0.020678542761339083\n",
      "Epoch: 348, Train Loss: 0.02008819641917944, Val Loss: 0.020710118262777254\n",
      "Epoch: 349, Train Loss: 0.020047529622912407, Val Loss: 0.020693002061711416\n",
      "Epoch: 350, Train Loss: 0.020023762203752994, Val Loss: 0.020814684441401846\n",
      "Epoch: 351, Train Loss: 0.02004524192586541, Val Loss: 0.02067618616043575\n",
      "Epoch: 352, Train Loss: 0.020039901457726956, Val Loss: 0.020667467532413348\n",
      "Epoch: 353, Train Loss: 0.0200529049821198, Val Loss: 0.020652070286728087\n",
      "Epoch: 354, Train Loss: 0.020033115804195405, Val Loss: 0.02061194290835706\n",
      "Epoch: 355, Train Loss: 0.02000229922309518, Val Loss: 0.02067713811993599\n",
      "Epoch: 356, Train Loss: 0.020047208625823258, Val Loss: 0.020653531191841004\n",
      "Epoch: 357, Train Loss: 0.02001684323698282, Val Loss: 0.020757483880198192\n",
      "Epoch: 358, Train Loss: 0.020012734808027743, Val Loss: 0.020624940209682027\n",
      "Epoch: 359, Train Loss: 0.02000916402041912, Val Loss: 0.020618836203264813\n",
      "Epoch: 360, Train Loss: 0.019990891363471746, Val Loss: 0.02061073798390608\n",
      "Epoch: 361, Train Loss: 0.02001417903602123, Val Loss: 0.020629853453664554\n",
      "Epoch: 362, Train Loss: 0.020018046360462904, Val Loss: 0.02061393969352283\n",
      "Epoch: 363, Train Loss: 0.019991761170327662, Val Loss: 0.020704831484527814\n",
      "Epoch: 364, Train Loss: 0.020021966334432362, Val Loss: 0.020602232288746608\n",
      "Epoch: 365, Train Loss: 0.01999993758648634, Val Loss: 0.020625425532223688\n",
      "Epoch: 366, Train Loss: 0.019991024762392045, Val Loss: 0.02059474750052369\n",
      "Epoch: 367, Train Loss: 0.01998082333430648, Val Loss: 0.020556942367601017\n",
      "Epoch: 368, Train Loss: 0.01997669880464673, Val Loss: 0.020573845665369715\n",
      "Epoch: 369, Train Loss: 0.019944675236940382, Val Loss: 0.020558335212251498\n",
      "Epoch: 370, Train Loss: 0.019949555825442077, Val Loss: 0.02056931940809129\n",
      "Epoch: 371, Train Loss: 0.01994309549406171, Val Loss: 0.02057900826727587\n",
      "Epoch: 372, Train Loss: 0.01996760293096304, Val Loss: 0.02068189676437113\n",
      "Epoch: 373, Train Loss: 0.019950466625392436, Val Loss: 0.020569387084198375\n",
      "Epoch: 374, Train Loss: 0.019955720756202937, Val Loss: 0.020602114853404817\n",
      "Epoch: 375, Train Loss: 0.019915489297360183, Val Loss: 0.020613986437046337\n",
      "Epoch: 376, Train Loss: 0.019935506515204905, Val Loss: 0.020694317532673714\n",
      "Epoch: 377, Train Loss: 0.019915979608893394, Val Loss: 0.020689673662658722\n",
      "Epoch: 378, Train Loss: 0.01993178730458021, Val Loss: 0.020554891506594324\n",
      "Epoch: 379, Train Loss: 0.019920859556645156, Val Loss: 0.020560695981932064\n",
      "Epoch: 380, Train Loss: 0.019920905262231826, Val Loss: 0.020587518781660093\n",
      "Epoch: 381, Train Loss: 0.019911995865404607, Val Loss: 0.02062739845779207\n",
      "Epoch: 382, Train Loss: 0.019922700691968203, Val Loss: 0.020591028419042392\n",
      "Epoch: 383, Train Loss: 0.01990923073142767, Val Loss: 0.020606282536709118\n",
      "Epoch: 384, Train Loss: 0.019873401526361703, Val Loss: 0.020621560395710052\n",
      "Epoch: 385, Train Loss: 0.01990807506069541, Val Loss: 0.020609582256939676\n",
      "Epoch: 386, Train Loss: 0.019887947551906107, Val Loss: 0.020583604388530293\n",
      "Epoch: 387, Train Loss: 0.019869651928544046, Val Loss: 0.020632915967513644\n",
      "Epoch: 388, Train Loss: 0.01986598949506879, Val Loss: 0.0205392548370929\n",
      "Epoch: 389, Train Loss: 0.019865877136588096, Val Loss: 0.020514692076378398\n",
      "Epoch: 390, Train Loss: 0.019852555461227894, Val Loss: 0.020582839610084655\n",
      "Epoch: 391, Train Loss: 0.019870051570236684, Val Loss: 0.020601492375135422\n",
      "Epoch: 392, Train Loss: 0.019861959077417852, Val Loss: 0.020544954235591585\n",
      "Epoch: 393, Train Loss: 0.019833671484142543, Val Loss: 0.020675300694410762\n",
      "Epoch: 394, Train Loss: 0.019843945018947123, Val Loss: 0.020510202687647607\n",
      "Epoch: 395, Train Loss: 0.019837684724479915, Val Loss: 0.02053092174705059\n",
      "Epoch: 396, Train Loss: 0.01984188788384199, Val Loss: 0.020596997989785103\n",
      "Epoch: 397, Train Loss: 0.01985496626794338, Val Loss: 0.020480207951059416\n",
      "Epoch: 398, Train Loss: 0.019847260419279335, Val Loss: 0.02056543623644208\n",
      "Epoch: 399, Train Loss: 0.019826261505484583, Val Loss: 0.02059357489148776\n",
      "Epoch: 400, Train Loss: 0.01984487946331501, Val Loss: 0.020489331778316272\n",
      "Epoch: 401, Train Loss: 0.019838788863271475, Val Loss: 0.020506243591034222\n",
      "Epoch: 402, Train Loss: 0.019813245832920073, Val Loss: 0.020555659685106503\n",
      "Epoch: 403, Train Loss: 0.01981660236045718, Val Loss: 0.020469745798479943\n",
      "Epoch: 404, Train Loss: 0.01980425649136305, Val Loss: 0.020555609038897922\n",
      "Epoch: 405, Train Loss: 0.019802566014230253, Val Loss: 0.02046579917863248\n",
      "Epoch: 406, Train Loss: 0.019838550444692373, Val Loss: 0.02051440097155079\n",
      "Epoch: 407, Train Loss: 0.019827441781759263, Val Loss: 0.020541969095430677\n",
      "Epoch: 408, Train Loss: 0.019774147413671016, Val Loss: 0.020466079166720785\n",
      "Epoch: 409, Train Loss: 0.01982673883065581, Val Loss: 0.02045835888693257\n",
      "Epoch: 410, Train Loss: 0.01980823688954115, Val Loss: 0.020427783566807942\n",
      "Epoch: 411, Train Loss: 0.019783300261944532, Val Loss: 0.02042074625690778\n",
      "Epoch: 412, Train Loss: 0.01975725484639406, Val Loss: 0.020425065287521908\n",
      "Epoch: 413, Train Loss: 0.019769676558673382, Val Loss: 0.020496229153303874\n",
      "Epoch: 414, Train Loss: 0.01978844854235649, Val Loss: 0.020483225051845824\n",
      "Epoch: 415, Train Loss: 0.01977719021588564, Val Loss: 0.02038308425200364\n",
      "Epoch: 416, Train Loss: 0.019747125793248416, Val Loss: 0.02054332113928265\n",
      "Epoch: 417, Train Loss: 0.019770822633057832, Val Loss: 0.020382046669958128\n",
      "Epoch: 418, Train Loss: 0.019742722049355507, Val Loss: 0.020517889262428358\n",
      "Epoch: 419, Train Loss: 0.019754737552255392, Val Loss: 0.02049166564312246\n",
      "Epoch: 420, Train Loss: 0.019784924253821373, Val Loss: 0.02052538324561384\n",
      "Epoch: 421, Train Loss: 0.019740999680012465, Val Loss: 0.020433758725486105\n",
      "Epoch: 422, Train Loss: 0.019709145568311215, Val Loss: 0.020392818199027152\n",
      "Epoch: 423, Train Loss: 0.019728404838591816, Val Loss: 0.020442295937784134\n",
      "Epoch: 424, Train Loss: 0.019745955843478442, Val Loss: 0.020413276369846058\n",
      "Epoch: 425, Train Loss: 0.019725002896040678, Val Loss: 0.020377889364248232\n",
      "Epoch: 426, Train Loss: 0.019720248959958553, Val Loss: 0.020422168489959504\n",
      "Epoch: 427, Train Loss: 0.01970596044510603, Val Loss: 0.020375276132235453\n",
      "Epoch: 428, Train Loss: 0.01972484839335084, Val Loss: 0.02038121788156411\n",
      "Epoch: 429, Train Loss: 0.019728414427489044, Val Loss: 0.020404023369626392\n",
      "Epoch: 430, Train Loss: 0.01973223140835762, Val Loss: 0.020451201865124325\n",
      "Epoch: 431, Train Loss: 0.019700849402695893, Val Loss: 0.020401410728929533\n",
      "Epoch: 432, Train Loss: 0.01968513911217451, Val Loss: 0.02033945956518726\n",
      "Epoch: 433, Train Loss: 0.019683434538543224, Val Loss: 0.020392792831574167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 434, Train Loss: 0.019705231145024298, Val Loss: 0.020337210702044622\n",
      "Epoch: 435, Train Loss: 0.019716062862426043, Val Loss: 0.020415882092146648\n",
      "Epoch: 436, Train Loss: 0.01968511263281107, Val Loss: 0.020373528143243183\n",
      "Epoch: 437, Train Loss: 0.01969240854680538, Val Loss: 0.020356427198128094\n",
      "Epoch: 438, Train Loss: 0.019668528076261283, Val Loss: 0.02033337347564243\n",
      "Epoch: 439, Train Loss: 0.019687837615609168, Val Loss: 0.020312878998026015\n",
      "Epoch: 440, Train Loss: 0.01970125022530556, Val Loss: 0.02030286689599355\n",
      "Epoch: 441, Train Loss: 0.01965828290209174, Val Loss: 0.02046931857272746\n",
      "Epoch: 442, Train Loss: 0.01965878043696284, Val Loss: 0.02034318692509144\n",
      "Epoch: 443, Train Loss: 0.019647445667535065, Val Loss: 0.020310359046099676\n",
      "Epoch: 444, Train Loss: 0.019640009097754957, Val Loss: 0.020348268989769238\n",
      "Epoch: 445, Train Loss: 0.01966239084303379, Val Loss: 0.02039063914072892\n",
      "Epoch: 446, Train Loss: 0.01966686099767685, Val Loss: 0.020345695819410068\n",
      "Epoch: 447, Train Loss: 0.01968598049879074, Val Loss: 0.02038892846377123\n",
      "Epoch: 448, Train Loss: 0.019649377107620238, Val Loss: 0.020327982330133044\n",
      "Epoch: 449, Train Loss: 0.01964793284982443, Val Loss: 0.0204051070151821\n",
      "Epoch: 450, Train Loss: 0.01961218886449933, Val Loss: 0.020457773779829342\n",
      "Epoch: 451, Train Loss: 0.0196402534507215, Val Loss: 0.020294199328100872\n",
      "Epoch: 452, Train Loss: 0.01961023259535432, Val Loss: 0.020311934784764334\n",
      "Epoch: 453, Train Loss: 0.019637454450130463, Val Loss: 0.020286739109054444\n",
      "Epoch: 454, Train Loss: 0.019611107610166073, Val Loss: 0.020331117132353405\n",
      "Epoch: 455, Train Loss: 0.019634415570646525, Val Loss: 0.020302296660485723\n",
      "Epoch: 456, Train Loss: 0.01961138493195176, Val Loss: 0.020299344722713743\n",
      "Epoch: 457, Train Loss: 0.019606740724295378, Val Loss: 0.0203213727190381\n",
      "Epoch: 458, Train Loss: 0.01957742715999484, Val Loss: 0.020267500320360773\n",
      "Epoch: 459, Train Loss: 0.01958861568570137, Val Loss: 0.020243336343102984\n",
      "Epoch: 460, Train Loss: 0.01957607465609908, Val Loss: 0.020476989625465302\n",
      "Epoch: 461, Train Loss: 0.019622894138097764, Val Loss: 0.02023993373390228\n",
      "Epoch: 462, Train Loss: 0.01957341818511486, Val Loss: 0.020310882301557632\n",
      "Epoch: 463, Train Loss: 0.01954794479906559, Val Loss: 0.020259025048405405\n",
      "Epoch: 464, Train Loss: 0.019589992865920065, Val Loss: 0.020271702033896295\n",
      "Epoch: 465, Train Loss: 0.019585129391402008, Val Loss: 0.02024196091270636\n",
      "Epoch: 466, Train Loss: 0.019593548148870467, Val Loss: 0.02051109057806787\n",
      "Epoch: 467, Train Loss: 0.019568331804126503, Val Loss: 0.020205514771597727\n",
      "Epoch: 468, Train Loss: 0.019543936479836703, Val Loss: 0.02025410689650074\n",
      "Epoch: 469, Train Loss: 0.01954537271335721, Val Loss: 0.020230169127148295\n",
      "Epoch: 470, Train Loss: 0.019580124370753765, Val Loss: 0.020264121984678602\n",
      "Epoch: 471, Train Loss: 0.019518503326922655, Val Loss: 0.02021791720910678\n",
      "Epoch: 472, Train Loss: 0.01954861381277442, Val Loss: 0.020330157840535754\n",
      "Epoch: 473, Train Loss: 0.019542283795773982, Val Loss: 0.020206533254139007\n",
      "Epoch: 474, Train Loss: 0.019537611469626428, Val Loss: 0.02018700681981586\n",
      "Epoch: 475, Train Loss: 0.019516855247318745, Val Loss: 0.02036503321003346\n",
      "Epoch: 476, Train Loss: 0.01951969139277935, Val Loss: 0.02032080035479296\n",
      "Epoch: 477, Train Loss: 0.01950756760686636, Val Loss: 0.020222514010374507\n",
      "Epoch: 478, Train Loss: 0.01952447585761547, Val Loss: 0.02020980203376403\n",
      "Epoch: 479, Train Loss: 0.01950713589042425, Val Loss: 0.020267784876364565\n",
      "Epoch: 480, Train Loss: 0.01951996584609151, Val Loss: 0.020191163061157105\n",
      "Epoch: 481, Train Loss: 0.019512175999581813, Val Loss: 0.020408039321265524\n",
      "Epoch: 482, Train Loss: 0.019517279986292123, Val Loss: 0.02021656974795319\n",
      "Epoch: 483, Train Loss: 0.019511215284466744, Val Loss: 0.02015354927806627\n",
      "Epoch: 484, Train Loss: 0.01944324577972293, Val Loss: 0.020139489027242812\n",
      "Epoch: 485, Train Loss: 0.0194886688105762, Val Loss: 0.02026391526063283\n",
      "Epoch: 486, Train Loss: 0.019474835719913243, Val Loss: 0.020143259834084246\n",
      "Epoch: 487, Train Loss: 0.01951324696838856, Val Loss: 0.020177362811943843\n",
      "Epoch: 488, Train Loss: 0.019477756828069688, Val Loss: 0.02019149696247445\n",
      "Epoch: 489, Train Loss: 0.0194737018533051, Val Loss: 0.020116787758611497\n",
      "Epoch: 490, Train Loss: 0.019470349077135324, Val Loss: 0.020156258569350317\n",
      "Epoch: 491, Train Loss: 0.019505656473338604, Val Loss: 0.020211462522782976\n",
      "Epoch: 492, Train Loss: 0.01944619384780526, Val Loss: 0.020150713238214688\n",
      "Epoch: 493, Train Loss: 0.01947894575074315, Val Loss: 0.020121293112872137\n",
      "Epoch: 494, Train Loss: 0.019477401349693536, Val Loss: 0.020081826352647374\n",
      "Epoch: 495, Train Loss: 0.019405976455658676, Val Loss: 0.02014192102092599\n",
      "Epoch: 496, Train Loss: 0.019457147806882857, Val Loss: 0.02014779444370005\n",
      "Epoch: 497, Train Loss: 0.019423552807420492, Val Loss: 0.020128078906545565\n",
      "Epoch: 498, Train Loss: 0.019422875486314297, Val Loss: 0.020121794134851486\n",
      "Epoch: 499, Train Loss: 0.01943317075818777, Val Loss: 0.020150969455402994\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.GaussianNLLLoss()\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for position, target_params in train_loader:\n",
    "        position = position.to(device).float()\n",
    "        target_params = target_params.to(device).float()\n",
    "        \n",
    "        pred_params_mu, pred_params_logvar = torch.split(model(position), 6, dim=1)\n",
    "        loss = loss_fn(pred_params_mu, target_params, torch.exp(pred_params_logvar))\n",
    "#         pred_params = model(position)\n",
    "#         loss = loss_fn(pred_params, target_params)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for position, target_params in val_loader:\n",
    "            position = position.to(device).float()\n",
    "            target_params = target_params.to(device).float()\n",
    "\n",
    "            pred_params_mu, pred_params_logvar = torch.split(model(position), 6, dim=1)\n",
    "            loss = loss_fn(pred_params_mu, target_params, torch.exp(pred_params_logvar))\n",
    "#             pred_params = model(position)\n",
    "#             loss = loss_fn(pred_params, target_params)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    print(\"Epoch: {}, Train Loss: {}, Val Loss: {}\".format(epoch, np.mean(train_losses), np.mean(val_losses)))\n",
    "    epoch_train_losses.append(np.mean(train_losses))\n",
    "    epoch_val_losses.append(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd8ebf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'AlexNet-Angle-500-Clipped10-GaussianLoss-1e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e38b0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses_1 = epoch_train_losses\n",
    "epoch_val_losses_1 = epoch_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b32f836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_train_losses)\n",
    "plt.plot(epoch_val_losses)\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa393d0",
   "metadata": {},
   "source": [
    "# Eyeball Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5f5debd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0088, -0.0780,  0.1418,  0.0033,  0.0514,  0.0021],\n",
      "       dtype=torch.float64)\n",
      "tensor([0.0560, 0.2993, 0.2533, 0.0578, 0.2702, 0.0392], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for position, target_params in val_loader:\n",
    "#     prediction = model(position.to(device).float())\n",
    "    pred_params_mu, pred_params_logvar = torch.split(model(position.to(device).float()), 6, dim=1)\n",
    "    prediction = pred_params_mu\n",
    "    r = prediction.detach().cpu() - target_params\n",
    "    break\n",
    "print(torch.mean(r, axis=0))\n",
    "print(torch.std(r, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045b024",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4928081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('FCNet-19-Clipped-GaussianLoss-1e-5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfda0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_positions = np.load('data/scaled_recorded_cloth_points.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff253594",
   "metadata": {},
   "source": [
    "## Using Cloth Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d04091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07699788 1.8899841  1.7827961  0.17709634 1.749825   1.7384199 ]\n"
     ]
    }
   ],
   "source": [
    "# Rescale and save\n",
    "recorded_positions = torch.as_tensor(recorded_positions).unsqueeze(0)\n",
    "pred_params, _ = torch.split(model(recorded_positions.to(device).float()), 6, dim=1)\n",
    "pred_params = pred_params.detach().cpu().numpy().squeeze()\n",
    "pred_params[0] = np.exp(pred_params[0])\n",
    "pred_params[3] /= 10\n",
    "print(pred_params)\n",
    "np.save('pred_params', pred_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bebec",
   "metadata": {},
   "source": [
    "## Using Joint Angles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52293c37",
   "metadata": {},
   "source": [
    "Use joint angles instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c4b5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qpos_2d(a, b, c, eps=1e-9):\n",
    "    u, v = a - b, c - b\n",
    "    u_norm, v_norm = np.linalg.norm(u) + eps, np.linalg.norm(v) + eps\n",
    "    phi = np.arccos(np.dot(u, v) / u_norm / v_norm)\n",
    "    th = np.pi - phi\n",
    "    u_hat, v_hat = np.block([u, 0]), np.block([v, 0])\n",
    "    w = np.cross(u_hat, v_hat)\n",
    "    th *= -1 if w[-1] > 0 else 1\n",
    "    return th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df447ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c, _ = recorded_positions.shape\n",
    "joint_angles = np.zeros((h, c - 2))\n",
    "for i in range(h):\n",
    "    for j in range(c - 2):\n",
    "        p1, p2, p3 = recorded_positions[i, j], recorded_positions[i, j + 1], recorded_positions[i, j + 2]\n",
    "        joint_angles[i, j] = qpos_2d(p1, p2, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8525062e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6756399 0.        1.        0.1       0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Rescale and save\n",
    "recorded_positions = torch.as_tensor(joint_angles).unsqueeze(0)\n",
    "# pred_params = model(recorded_positions.to(device).float())\n",
    "pred_params, _ = torch.split(model(recorded_positions.to(device).float()), 6, dim=1)\n",
    "pred_params = pred_params.detach().cpu().numpy().squeeze()\n",
    "pred_params[0] = np.exp(pred_params[0])\n",
    "pred_params[3] /= 10\n",
    "print(pred_params)\n",
    "np.save('pred_params', pred_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976add8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
